id,should_trigger,prompt,sandbox,timeout_s,max_commands,output_schema,judge,require_files,must_include,must_not_include,fixture
explicit,true,"You are being evaluated. If you use any skill(s), list their names in `invoked_skills`.

Return a JSON object that matches the provided output schema.

Use the $uv-tensorrt-llm skill.

Task: Provide the smallest correct, actionable response for a realistic scenario covered by the skill.
Constraints: do not execute commands; do not write files.

Scenario (from skill description): Optimizes LLM inference with NVIDIA TensorRT for maximum throughput and lowest latency. Use for production deployment on NVIDIA GPUs (A100/H100), when you need 10-100x faster inference than PyTorch, or for serving models with quantization (FP8/INT4), in-flight batching, and multi-GPU scaling.
",read-only,120,0,evals/schemas/skill_response.schema.json,true,,"\""uv-tensorrt-llm\""",,
implicit,true,"You are being evaluated. If you use any skill(s), list their names in `invoked_skills`.

Return a JSON object that matches the provided output schema.

Do not mention any skill name.

Task: Respond as you would in a real session.
Constraints: do not execute commands; do not write files.

User request: Optimizes LLM inference with NVIDIA TensorRT for maximum throughput and lowest latency. Use for production deployment on NVIDIA GPUs (A100/H100), when you need 10-100x faster inference than PyTorch, or for serving models with quantization (FP8/INT4), in-flight batching, and multi-GPU scaling.
",read-only,120,0,evals/schemas/skill_response.schema.json,true,,"\""uv-tensorrt-llm\""",,
negative,false,"You are being evaluated. If you use any skill(s), list their names in `invoked_skills`.

Return a JSON object that matches the provided output schema.

Do not mention any skill name.

User request: Give me a 1-sentence explanation of what a binary search is.
Constraints: do not execute commands; do not write files.
",read-only,90,0,evals/schemas/skill_response.schema.json,true,,,"\""uv-tensorrt-llm\""",
