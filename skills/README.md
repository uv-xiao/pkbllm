# Skills (distribution)

This folder is the distribution surface for the Skills CLI (`npx skills`).

- Each direct child directory is a skill.
- Contents are generated from the canonical categorized directories:
  `knowledge/`, `productivity/`, and `human/`.

Do not edit skills here by hand; edit the canonical skill and re-sync.

## <TABLE>
<!-- PKBLLM_TABLE_START -->
| Path | Type | Description |
| --- | --- | --- |
| `manifest.json` | file | Data file |
| `uv-bootstrap-ml-knowledge-authoring/` | skill | Create and curate new ML domain knowledge skills in this repo. Use when adding a new `knowledge/ML/*` skill, extending the curated ML taxonomy (model-architecture, training, distributed, serving, paper, kernel, agents), scaffolding a new skill folder, and ensuring naming (`uv-*`), licensing, and the generated `skills/` mirror stay consistent. |
| `uv-bootstrap-skill-maintenance/` | skill | Maintain and curate the pkbllm skills repository. Use when adding/importing a new skill, merging skills from external repos, updating or refactoring existing skills, regenerating the generated `skills/` mirror, or ensuring licensing/compliance and naming conventions (all skills must start with `uv-`). |
| `uv-brainstorming/` | skill | You MUST use this before any creative work - creating features, building components, adding functionality, or modifying behavior. Explores user intent, requirements and design before implementation. |
| `uv-content-prompts/` | skill | Convert raw material into per-page Content PROMPTs by analyzing content density, intent, and slide usage. Outputs $HUMAN_MATERIAL_PATH/slides/<deck>/prompts/content/<deck>.md. Use when the user has notes/materials and wants a well-planned per-page content prompt before styling. |
| `uv-create-paper-exercises/` | skill | Create learning exercises from a research paper (arXiv URL/PDF). Use when turning a paper into (1) a programming exercise (extract the core technique into a coding problem with tests) and (2) a modeling exercise (extract formulas/reasoning into calculation problems with worked solutions). Generates an exercise pack under $HUMAN_MATERIAL_PATH/exercises/<paper_slug>/ including local mini-skills to check answers and reveal golden solutions. |
| `uv-deepspeed/` | skill | Expert guidance for distributed training with DeepSpeed - ZeRO optimization stages, pipeline parallelism, FP16/BF16/FP8, 1-bit Adam, sparse attention |
| `uv-dispatching-parallel-agents/` | skill | Use when facing 2+ independent tasks that can be worked on without shared state or sequential dependencies |
| `uv-distributed-llm-pretraining-torchtitan/` | skill | Provides PyTorch-native distributed LLM pretraining using torchtitan with 4D parallelism (FSDP2, TP, PP, CP). Use when pretraining Llama 3.1, DeepSeek V3, or custom models at scale from 8 to 512+ GPUs with Float8, torch.compile, and distributed checkpointing. |
| `uv-executing-plans/` | skill | Use when you have a written implementation plan to execute in a separate session with review checkpoints |
| `uv-find-skills/` | skill | Helps users discover and install agent skills when they ask questions like "how do I do X", "find a skill for X", "is there a skill that can...", or express interest in extending capabilities. This skill should be used when the user is looking for functionality that might exist as an installable skill. |
| `uv-finishing-a-development-branch/` | skill | Use when implementation is complete, all tests pass, and you need to decide how to integrate the work - guides completion of development work by presenting structured options for merge, PR, or cleanup |
| `uv-implementing-llms-litgpt/` | skill | Implements and trains LLMs using Lightning AI's LitGPT with 20+ pretrained architectures (Llama, Gemma, Phi, Qwen, Mistral). Use when need clean model implementations, educational understanding of architectures, or production fine-tuning with LoRA/QLoRA. Single-file implementations, no abstraction layers. |
| `uv-init-human-material-repo/` | skill | Initialize a dedicated HUMAN_MATERIAL_PATH git repository for generated human-facing materials. Use when a user asks to set up a new materials repo/folder for slides/manuscripts/exercises, create the expected file structure under $HUMAN_MATERIAL_PATH, and create a local-only .OPENROUTER_API_KEY file for slider rendering. |
| `uv-literature-review/` | skill | Conduct comprehensive literature reviews (systematic/narrative/scoping) across multiple databases, synthesize findings, and produce a well-cited review document. Use when planning and writing literature reviews or state-of-the-art surveys; prefer outputs under $HUMAN_MATERIAL_PATH/research/<topic>/. |
| `uv-llama-cpp/` | skill | Runs LLM inference on CPU, Apple Silicon, and consumer GPUs without NVIDIA hardware. Use for edge deployment, M1/M2/M3 Macs, AMD/Intel GPUs, or when CUDA is unavailable. Supports GGUF quantization (1.5-8 bit) for reduced memory and 4-10× speedup vs PyTorch on CPU. |
| `uv-mamba-architecture/` | skill | State-space model with O(n) complexity vs Transformers' O(n²). 5× faster inference, million-token sequences, no KV cache. Selective SSM with hardware-aware design. Mamba-1 (d_state=16) and Mamba-2 (d_state=128, multi-head). Models 130M-2.8B on HuggingFace. |
| `uv-miles-rl-training/` | skill | Provides guidance for enterprise-grade RL training using miles, a production-ready fork of slime. Use when training large MoE models with FP8/INT4, needing train-inference alignment, or requiring speculative RL for maximum throughput. |
| `uv-ml-paper-writing/` | skill | Write publication-ready ML/AI papers for NeurIPS, ICML, ICLR, ACL, AAAI, COLM. Use when drafting papers from research repos, structuring arguments, verifying citations, or preparing camera-ready submissions. Includes LaTeX templates, reviewer guidelines, and citation verification workflows. |
| `uv-moe-training/` | skill | Train Mixture of Experts (MoE) models using DeepSpeed or HuggingFace. Use when training large-scale models with limited compute (5× cost reduction vs dense models), implementing sparse architectures like Mixtral 8x7B or DeepSeek-V3, or scaling model capacity without proportional compute increase. Covers MoE architectures, routing mechanisms, load balancing, expert parallelism, and inference optimization. |
| `uv-nanogpt/` | skill | Educational GPT implementation in ~300 lines. Reproduces GPT-2 (124M) on OpenWebText. Clean, hackable code for learning transformers. By Andrej Karpathy. Perfect for understanding GPT architecture from scratch. Train on Shakespeare (CPU) or OpenWebText (multi-GPU). |
| `uv-pytorch-fsdp2/` | skill | Adds PyTorch FSDP2 (fully_shard) to training scripts with correct init, sharding, mixed precision/offload config, and distributed checkpointing. Use when models exceed single-GPU memory or when you need DTensor-based sharding with DeviceMesh. |
| `uv-ray-train/` | skill | Distributed training orchestration across clusters. Scales PyTorch/TensorFlow/HuggingFace from laptop to 1000s of nodes. Built-in hyperparameter tuning with Ray Tune, fault tolerance, elastic scaling. Use when training massive models across multiple machines or running distributed hyperparameter sweeps. |
| `uv-read-arxiv-paper/` | skill | Download and deeply read an arXiv paper (given an arXiv URL or id), then write a clear human-facing report with strong storytelling and logical reasoning. Use when asked to summarize/review an arXiv paper, extract key ideas, connect them to practice, and produce a report under $HUMAN_MATERIAL_PATH/research/<paper_slug>/report.md. Stores downloads under $HUMAN_MATERIAL_PATH/.references/ (configurable via $HUMAN_MATERIAL_PATH/.agents/config.toml or ~/.agents/config.toml). |
| `uv-receiving-code-review/` | skill | Use when receiving code review feedback, before implementing suggestions, especially if feedback seems unclear or technically questionable - requires technical rigor and verification, not performative agreement or blind implementation |
| `uv-requesting-code-review/` | skill | Use when completing tasks, implementing major features, or before merging to verify work meets requirements |
| `uv-rwkv-architecture/` | skill | RNN+Transformer hybrid with O(n) inference. Linear time, infinite context, no KV cache. Train like GPT (parallel), infer like RNN (sequential). Linux Foundation AI project. Production at Windows, Office, NeMo. RWKV-7 (March 2025). Models up to 14B parameters. |
| `uv-scientific-schematics/` | skill | Create publication-quality scientific diagrams via OpenRouter image models with smart iterative refinement and automated quality review. Use when generating figures for papers/reports/slides (architectures, system diagrams, flowcharts, pathways). Prefer saving outputs under $HUMAN_MATERIAL_PATH/research/<topic>/figures/. |
| `uv-scientific-writing/` | skill | Write and revise scientific manuscripts in full paragraphs (not bullet points), using a two-stage workflow (outline → prose). Use when drafting IMRAD sections, applying reporting guidelines (CONSORT/STROBE/PRISMA), formatting citations (APA/AMA/Vancouver), and producing publishable writing in the HUMAN materials repo (usually under $HUMAN_MATERIAL_PATH/manuscripts/ or $HUMAN_MATERIAL_PATH/research/). |
| `uv-serving-llms-vllm/` | skill | Serves LLMs with high throughput using vLLM's PagedAttention and continuous batching. Use when deploying production LLM APIs, optimizing inference latency/throughput, or serving models with limited GPU memory. Supports OpenAI-compatible endpoints, quantization (GPTQ/AWQ/FP8), and tensor parallelism. |
| `uv-sglang/` | skill | Fast structured generation and serving for LLMs with RadixAttention prefix caching. Use for JSON/regex outputs, constrained decoding, agentic workflows with tool calls, or when you need 5× faster inference than vLLM with prefix sharing. Powers 300,000+ GPUs at xAI, AMD, NVIDIA, and LinkedIn. |
| `uv-slider-plan/` | skill | Plan the slider workflow end-to-end by selecting which repo skills to run (content-prompts, styled-prompts, styled-artifacts) based on the user’s starting input (materials or existing prompts) and requested output (content prompt, styled prompt, images, PDF, PPTX). Uses $HUMAN_MATERIAL_PATH/slides/<deck>/ as the working root. |
| `uv-slime-rl-training/` | skill | Provides guidance for LLM post-training with RL using slime, a Megatron+SGLang framework. Use when training GLM models, implementing custom data generation workflows, or needing tight Megatron-LM integration for RL scaling. |
| `uv-speculative-decoding/` | skill | Accelerate LLM inference using speculative decoding, Medusa multiple heads, and lookahead decoding techniques. Use when optimizing inference speed (1.5-3.6× speedup), reducing latency for real-time applications, or deploying models with limited compute. Covers draft models, tree-based attention, Jacobi iteration, parallel token generation, and production deployment strategies. |
| `uv-styled-artifacts/` | skill | Generate slide images and final PDF/PPTX from v2 styled prompts ($HUMAN_MATERIAL_PATH/slides/<deck>/prompts/styled/<deck>.md), storing intermediates in $HUMAN_MATERIAL_PATH/slides/<deck>/artifacts/<deck>/work/. Use when the user asks to render/generate/export slides from a Styled PROMPT into images/PDF/PPTX. |
| `uv-styled-prompts/` | skill | Convert per-page Content PROMPTs into design-complete Styled PROMPTs using a Markdown style brief, inferring the best layout per page during creation. Outputs $HUMAN_MATERIAL_PATH/slides/<deck>/prompts/styled/<deck>.md, ready for image/PDF/PPT generation. |
| `uv-subagent-driven-development/` | skill | Use when executing implementation plans with independent tasks in the current session |
| `uv-systematic-debugging/` | skill | Use when encountering any bug, test failure, or unexpected behavior, before proposing fixes |
| `uv-tensorrt-llm/` | skill | Optimizes LLM inference with NVIDIA TensorRT for maximum throughput and lowest latency. Use for production deployment on NVIDIA GPUs (A100/H100), when you need 10-100x faster inference than PyTorch, or for serving models with quantization (FP8/INT4), in-flight batching, and multi-GPU scaling. |
| `uv-test-driven-development/` | skill | Use when implementing any feature or bugfix, before writing implementation code |
| `uv-using-git-worktrees/` | skill | Use when starting feature work that needs isolation from current workspace or before executing implementation plans - creates isolated git worktrees with smart directory selection and safety verification |
| `uv-using-pkb/` | skill | Use pkbllm skills effectively. Use at the start of a session when working from a pkbllm repo checkout: discover which `uv-*` skill to invoke, understand the canonical-vs-generated layout (don’t edit `skills/`), install/list skills via Skills-CLI, and follow HUMAN_MATERIAL_PATH conventions (slides/research/exercises plus .references/ downloads with config.toml overrides). |
| `uv-using-superpowers/` | skill | Use when starting any conversation - establishes how to find and use skills, requiring Skill tool invocation before ANY response including clarifying questions |
| `uv-verification-before-completion/` | skill | Use when about to claim work is complete, fixed, or passing, before committing or creating PRs - requires running verification commands and confirming output before making any success claims; evidence before assertions always |
| `uv-verl-rl-training/` | skill | Provides guidance for training LLMs with reinforcement learning using verl (Volcano Engine RL). Use when implementing RLHF, GRPO, PPO, or other RL algorithms for LLM post-training at scale with flexible infrastructure backends. |
| `uv-writing-plans/` | skill | Use when you have a spec or requirements for a multi-step task, before touching code |
| `uv-writing-skills/` | skill | Use when creating new skills, editing existing skills, or verifying skills work before deployment |
<!-- PKBLLM_TABLE_END -->
